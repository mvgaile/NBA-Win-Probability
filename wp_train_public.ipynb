{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6620ce40",
   "metadata": {},
   "source": [
    "# NBA Win Probability Model â€” Training Pipeline\n",
    "\n",
    "This notebook contains the complete end-to-end pipeline used to build the NBA Win Probability Model, including data ingestion, data engineering, feature creation, model training, calibration, and evaluation. It produces the final neural-network model and calibration objects used in the real-time win probability generator.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Overview\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. **Loads play-by-play and betting data**  \n",
    "   - Aggregates multi-season PBP files  \n",
    "   - Loads sportsbook moneylines  \n",
    "   - Merges PBP events with pregame and in-game betting information  \n",
    "\n",
    "2. **Cleans and standardizes raw data**  \n",
    "   - Fixes incorrect or missing score fields  \n",
    "   - Forward-fills score and possession state  \n",
    "   - Removes malformed or outlier sportsbook lines  \n",
    "   - Corrects time representations and handles overtime periods  \n",
    "\n",
    "3. **Constructs game-state features**  \n",
    "   - `seconds_remaining_game` (regulation + OT)  \n",
    "   - `score_diff`, `score_home`, `score_away`  \n",
    "   - Possession indicators  \n",
    "   - Free throw remaining indicators  \n",
    "   - Clutch markers  \n",
    "   - Log and inverse time transforms (`t_log`, `t_inv`)  \n",
    "\n",
    "4. **Integrates betting-derived priors**  \n",
    "   - Converts moneylines to implied probabilities  \n",
    "   - Averages across books  \n",
    "   - Attaches pregame implied win probability to every state  \n",
    "   - Cleans out extreme or erroneous sportsbook entries  \n",
    "\n",
    "5. **Builds the final training dataset**  \n",
    "   - One row per game state  \n",
    "   - Feature columns defined consistently  \n",
    "   - Label = final game outcome (home win = 1, away win = 0)  \n",
    "   - Optional grouping to prevent leakage across games  \n",
    "\n",
    "6. **Scales and splits the data**  \n",
    "   - Normalizes continuous features  \n",
    "   - Train/validation split using `GroupShuffleSplit`  \n",
    "   - Ensures full games stay together in each partition  \n",
    "\n",
    "7. **Defines and trains the neural network model**  \n",
    "   - Custom `MLP` architecture in PyTorch  \n",
    "   - Trains with Adam and BCE loss  \n",
    "   - Uses validation metrics to tune training length  \n",
    "\n",
    "8. **Calibrates the model**  \n",
    "   - Wraps raw model outputs using Isotonic Regression  \n",
    "   - Produces better-calibrated probabilities  \n",
    "   - Saves auxiliary objects (`scaler`, `iso`, `feature_cols`)  \n",
    "\n",
    "9. **Evaluates model quality**  \n",
    "   - Brier score  \n",
    "   - Log loss  \n",
    "   - ROC AUC  \n",
    "   - Probability histograms  \n",
    "   - Reliability and calibration curves  \n",
    "\n",
    "10. **Exports artifacts for inference notebook**  \n",
    "    - `nn_winprob_model.pt` (trained PyTorch model)  \n",
    "    - `nn_winprob_aux.pkl` (scaler, calibrator, feature list)  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Output of This Notebook\n",
    "\n",
    "After running the full pipeline, the following artifacts are produced:\n",
    "\n",
    "- **`nn_winprob_model.pt`**  \n",
    "  Trained PyTorch state dict for the MLP model.\n",
    "\n",
    "- **`nn_winprob_aux.pkl`**  \n",
    "  Contains:\n",
    "  - `scaler` (StandardScaler fit on training data)  \n",
    "  - `iso` (IsotonicRegression calibrator)  \n",
    "  - `feature_cols` (ordered list of model input features)  \n",
    "\n",
    "These files are consumed by the separate **single-game win probability generator** notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Purpose of This Notebook\n",
    "\n",
    "This file is the authoritative reference for how the win probability model is engineered and trained. It contains:\n",
    "\n",
    "- All transformations  \n",
    "- All model logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Handling ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Machine Learning / Modeling ===\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss, log_loss, roc_auc_score\n",
    "\n",
    "# === NBA API ===\n",
    "from nba_api.stats.endpoints import PlayByPlayV3, boxscoresummaryv2\n",
    "\n",
    "# === Utilities ===\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a22ad",
   "metadata": {},
   "source": [
    "## âš ï¸ Data Download Notes\n",
    "\n",
    "The first section of this notebook downloads all required play-by-play and game summary data from the NBA Stats API. This step **takes a very long time** and, due to API rate limiting, **cannot be completed in a single uninterrupted run**. After approximately 600 requests, the NBA API typically stops responding or begins returning errors.\n",
    "\n",
    "To work around this, the download process is intentionally designed to:\n",
    "- fetch data in **batches**,  \n",
    "- **resume** from partially completed progress, and  \n",
    "- **cache** previously downloaded files to avoid repeated API calls.\n",
    "\n",
    "If you already have the cached data available, you can safely skip the download step and proceed directly to data engineering. Make sure to edit directories to match.\n",
    "\n",
    "This file only downloads games that have betting data available. This data was collected from Kaggle. Both nba_betting_money_line.csv and nba_games_all.csv are needed for the project, but only the former is needed to download the games. \n",
    " - https://www.kaggle.com/datasets/ehallmar/nba-historical-stats-and-betting-data/discussion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42013c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mvgai\\SportsAnalytics\\csvs\\nba_betting_money_line.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Config\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\".\"))\n",
    "ODDS_CSV_PATH = os.path.join(BASE_DIR, \"csvs\", \"nba_betting_money_line.csv\") # Directory to fetch betting odds CSV\n",
    "PBP_OUT_DIR = os.path.join(BASE_DIR, \"pbp_raw\") # Directory to save raw PBP data\n",
    "PROGRESS_START = 0  # For resuming partial runs\n",
    "SLEEP_SECONDS = 0.25   # Gentle rate limit padding\n",
    "\n",
    "# For testing: set to an int (e.g. 20). For full run: set to None.\n",
    "DOWNLOAD_LIMIT = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660665c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def load_unique_game_ids(csv_path: str) -> list[str]:\n",
    "    \"\"\"Load and normalize unique game IDs from the odds CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"game_id\" in df.columns:\n",
    "        col = \"game_id\"\n",
    "    elif \"gameId\" in df.columns:\n",
    "        col = \"gameId\"\n",
    "    else:\n",
    "        raise ValueError(\"Could not find game_id or gameId column in odds CSV.\")\n",
    "\n",
    "    ids = (\n",
    "        df[col]\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "        .astype(str)\n",
    "        .map(lambda x: x.zfill(10))\n",
    "        .unique()\n",
    "    )\n",
    "    id_list = list(ids)\n",
    "    return id_list[PROGRESS_START:]\n",
    "\n",
    "\n",
    "def ensure_output_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def pbp_output_path(game_id: str) -> str:\n",
    "    # Save as CSV (no parquet engine needed)\n",
    "    return os.path.join(PBP_OUT_DIR, f\"{game_id}.csv\")\n",
    "\n",
    "\n",
    "def download_single_game(game_id: str) -> bool:\n",
    "    \"\"\"\n",
    "    Download one game's PBP. Returns True on success, False on failure.\n",
    "    \"\"\"\n",
    "    out_path = pbp_output_path(game_id)\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        # already cached\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        pbp = PlayByPlayV3(game_id=game_id)\n",
    "        df = pbp.get_data_frames()[0]\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"\\n[WARN] Empty PBP for game {game_id}\")\n",
    "            return False\n",
    "\n",
    "        # Write as CSV instead of parquet\n",
    "        df.to_csv(out_path, index=False)\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed for game {game_id}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091fcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Main - DO NOT RUN UNLESS READY TO DOWNLOAD DATA\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    start = time.time()\n",
    "    print(f\"Loading game IDs from {ODDS_CSV_PATH} ...\")\n",
    "\n",
    "    game_ids = load_unique_game_ids(ODDS_CSV_PATH)\n",
    "    total = len(game_ids)\n",
    "    print(f\"Found {total} games.\")\n",
    "\n",
    "    ensure_output_dir(PBP_OUT_DIR)\n",
    "\n",
    "    num_ok = 0\n",
    "    num_fail = 0\n",
    "\n",
    "    for idx, gid in enumerate(game_ids, start=1):\n",
    "        if DOWNLOAD_LIMIT is not None and idx > DOWNLOAD_LIMIT:\n",
    "            break\n",
    "\n",
    "        # Single-line progress update\n",
    "        progress = f\"[{idx}/{total}] Downloading PBP for {gid} ...\"\n",
    "        sys.stdout.write(\"\\r\" + progress)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        success = download_single_game(gid)\n",
    "        if success:\n",
    "            num_ok += 1\n",
    "        else:\n",
    "            num_fail += 1\n",
    "\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "\n",
    "    # Final newline to clear the progress line\n",
    "    print()\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"Completed in {elapsed:.2f} seconds.\")\n",
    "    print(\"\\n[SUMMARY]\")\n",
    "    print(f\"  Total games considered:   {min(total, DOWNLOAD_LIMIT or total)}\")\n",
    "    print(f\"  Successfully downloaded:  {num_ok}\")\n",
    "    print(f\"  Failed / errors:          {num_fail}\")\n",
    "    print(f\"  Output directory:         {PBP_OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483d907",
   "metadata": {},
   "source": [
    "## ðŸ·ï¸ Betting Odds Standardization\n",
    "\n",
    "The sportsbook odds dataset is not consistent in how it labels the **home** and **away** teams across different entries. Some rows list the home team in the first column, others label teams in reverse order, and a few books use their own internal formatting.\n",
    "\n",
    "This section cleans and standardizes the odds data by:\n",
    "- correctly identifying which team is the **home** team for each game,  \n",
    "- aligning all rows to a consistent `(home_team, away_team)` format,  \n",
    "- ensuring moneylines always correspond to the correct team, and  \n",
    "- producing unified `home_win_implied_prob` and `away_win_implied_prob` fields.\n",
    "\n",
    "After this step, all betting information is aligned with the PBP data and ready for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "betting_data = pd.read_csv(\"csvs/nba_betting_money_line.csv\")\n",
    "games_all = pd.read_csv(\"csvs/nba_games_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter rows where 'matchup' contains an '@' sign using regex\n",
    "filtered_data = games_all[games_all['matchup'].str.contains(r'@', regex=True, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b50995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust home and away teams in betting data if needed\n",
    "\n",
    "home_away_dict = games_all.set_index('game_id')[['team_id', 'is_home']].apply(tuple, axis=1).to_dict()\n",
    "\n",
    "# Step 3: Define a function to adjust team_id and a_team_id based on 'is_home'\n",
    "def adjust_team_and_prices(row):\n",
    "    # Get the game_id for the current row\n",
    "    game_id = row['game_id']\n",
    "    team_id = row['team_id']\n",
    "    # Get the is_home value from the dictionary\n",
    "    is_home = home_away_dict.get(game_id)[1]\n",
    "    d_team_id = home_away_dict.get(game_id)[0]\n",
    "    \n",
    "    if team_id == d_team_id and is_home == 'f':\n",
    "        print(\"swapping for game_id:\", game_id)\n",
    "        row['team_id'], row['a_team_id'] = row['a_team_id'], row['team_id']\n",
    "        row['price1'], row['price2'] = row['price2'], row['price1']\n",
    "           \n",
    "    if team_id != d_team_id and is_home == 't':\n",
    "        print(\"swapping for game_id:\", game_id)\n",
    "        row['team_id'], row['a_team_id'] = row['a_team_id'], row['team_id']\n",
    "        row['price1'], row['price2'] = row['price2'], row['price1']\n",
    "    \n",
    "    # Return the adjusted row\n",
    "    return row\n",
    "\n",
    "# Step 4: Apply the adjustment function to each row in the betting data\n",
    "betting_data_adjusted = betting_data.apply(adjust_team_and_prices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f97fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust if needed\n",
    "ADJUSTED_ODDS_CSV_PATH = os.path.join(BASE_DIR, \"csvs\", \"adjusted_betting_data.csv\")\n",
    "betting_data_adjusted.to_csv(ADJUSTED_ODDS_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a3186",
   "metadata": {},
   "source": [
    "## Data Cleaning and Game State Creation\n",
    "\n",
    "### ðŸ§¹ Data Cleaning\n",
    "\n",
    "This section prepares the raw play-by-play and odds data so it can be reliably used for feature engineering and model training. The NBA Stats API often returns incomplete or irregular fields (such as missing scores, empty strings, inconsistent possession markers, or duplicated events), and the betting dataset includes occasional malformed or extreme values.\n",
    "\n",
    "Key cleaning steps include:\n",
    "- fixing missing or blank score fields using forward-fill logic,  \n",
    "- correcting inconsistent possession or event markers,  \n",
    "- reconciling regulation and overtime time formats,  \n",
    "- removing or capping extreme sportsbook outliers, and  \n",
    "- ensuring all numeric fields are properly typed and aligned.\n",
    "\n",
    "After this cleaning stage, the dataset has consistent structure and values, making downstream feature construction and modeling stable and reproducible.\n",
    "\n",
    "### ðŸ”„ Parsing Play-by-Play into Game States\n",
    "\n",
    "Raw play-by-play data from the NBA Stats API arrives as a long list of individual eventsâ€”shots, turnovers, rebounds, fouls, substitutions, etc. While this format is useful for descriptive analysis, it is **not directly usable** for win probability modeling. The model requires a unified representation of the game at each moment in time.\n",
    "\n",
    "In this section, the event-level PBP data is transformed into a sequence of **game states**, where each state represents a snapshot of the game after every meaningful update (score change, possession change, or time change). This process involves:\n",
    "\n",
    "- extracting scores, possession, period, and clock at each event,  \n",
    "- computing `seconds_remaining_game` across regulation and overtime,  \n",
    "- carrying forward score and possession information between events,  \n",
    "- identifying meaningful transitions (e.g., new possession, score update), and  \n",
    "- constructing one clean row per game state with synchronized fields.\n",
    "\n",
    "The result is a structured, chronological timeline of the game that the model can interpretâ€”each row holding all features needed to estimate the probability of the home team winning at that specific moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_scores_carry_forward(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each game, carry forward the last non-zero scoreHome/scoreAway.\n",
    "    Assumes df is already sorted in chronological PBP order within each game.\n",
    "    Recomputes score_diff at the end.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    score_cols = [\"scoreHome\", \"scoreAway\"]  # or whatever your score columns are\n",
    "\n",
    "    for col in score_cols:\n",
    "        # Convert to numeric, turn '' and other junk into NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    \n",
    "        # Forward fill within the game, then fill any leading NaNs with 0\n",
    "        df[col] = df[col].ffill().fillna(0).astype(int)\n",
    "    # Decide how to group (per game or whole df)\n",
    "    if \"gameId\" in df.columns:\n",
    "        group_cols = [\"gameId\"]\n",
    "    elif \"game_id\" in df.columns:\n",
    "        group_cols = [\"game_id\"]\n",
    "    else:\n",
    "        group_cols = None  # assume df is already a single game\n",
    "\n",
    "    def _fix_one_game(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        # If you want to be extra safe, you can sort inside each game\n",
    "        # by period/time/event_num, e.g.:\n",
    "        # sort_cols = [c for c in [\"period\", \"event_num\"] if c in g.columns]\n",
    "        # if sort_cols:\n",
    "        #     g = g.sort_values(sort_cols)\n",
    "\n",
    "        for col in [\"scoreHome\", \"scoreAway\"]:\n",
    "            if col in g.columns:\n",
    "                g[col] = (\n",
    "                    g[col]\n",
    "                    .replace(0, np.nan)  # treat 0 as \"missing\" between scores\n",
    "                    .ffill()             # carry forward last known score\n",
    "                    .fillna(0)           # before first score, keep 0-0\n",
    "                    .astype(int)\n",
    "                )\n",
    "\n",
    "        # Recompute score_diff if present / desired\n",
    "        if \"scoreHome\" in g.columns and \"scoreAway\" in g.columns:\n",
    "            g[\"score_diff\"] = g[\"scoreHome\"] - g[\"scoreAway\"]\n",
    "\n",
    "        return g\n",
    "\n",
    "    if group_cols is None:\n",
    "        df = _fix_one_game(df)\n",
    "    else:\n",
    "        df = df.groupby(group_cols, group_keys=False).apply(_fix_one_game)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def american_to_implied_prob(odds):\n",
    "    \"\"\"\n",
    "    Convert American odds to implied probability (no vig removal).\n",
    "    \"\"\"\n",
    "    if pd.isna(odds):\n",
    "        return np.nan\n",
    "\n",
    "    odds = float(odds)\n",
    "\n",
    "    if odds < 0:\n",
    "        return (-odds) / ((-odds) + 100)\n",
    "    else:\n",
    "        return 100 / (odds + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf72c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_iso_clock_to_seconds(clock_str) -> int:\n",
    "    \"\"\"\n",
    "    Parse PlayByPlayV3 'clock' strings like 'PT11M54.00S' into seconds.\n",
    "    \"\"\"\n",
    "    if clock_str is None or pd.isna(clock_str):\n",
    "        return 0\n",
    "\n",
    "    s = str(clock_str).strip()\n",
    "    if not s.startswith(\"PT\"):\n",
    "        return 0\n",
    "\n",
    "    # Strip 'PT' and trailing 'S'\n",
    "    s = s[2:]\n",
    "    if s.endswith(\"S\"):\n",
    "        s = s[:-1]\n",
    "\n",
    "    # Split on 'M'\n",
    "    if \"M\" not in s:\n",
    "        return 0\n",
    "\n",
    "    minutes_str, seconds_str = s.split(\"M\", 1)\n",
    "\n",
    "    try:\n",
    "        minutes = int(float(minutes_str))\n",
    "    except ValueError:\n",
    "        minutes = 0\n",
    "\n",
    "    try:\n",
    "        seconds = int(float(seconds_str))\n",
    "    except ValueError:\n",
    "        seconds = 0\n",
    "\n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add:\n",
    "      - seconds_remaining_in_period: time left in the current period\n",
    "      - seconds_remaining_game: time left until this period ends, assuming\n",
    "        the game ends at 0:00 of this period (i.e., end of Q4 or an OT)\n",
    "\n",
    "    For periods 1â€“4, seconds_remaining_game is \"time left in regulation\".\n",
    "    For periods > 4 (OTs), seconds_remaining_game is just the clock time left\n",
    "    in that OT period.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Seconds remaining in current period from the ISO clock\n",
    "    df[\"seconds_remaining_in_period\"] = df[\"clock\"].apply(parse_iso_clock_to_seconds)\n",
    "\n",
    "    period_length = 12 * 60        # 12-minute quarters\n",
    "    total_reg = 4 * period_length  # 48 minutes\n",
    "\n",
    "    def compute_seconds_remaining_game(row):\n",
    "        period = row[\"period\"]\n",
    "        sec_left = row[\"seconds_remaining_in_period\"]\n",
    "\n",
    "        if period <= 4:\n",
    "            # Time elapsed before this period\n",
    "            elapsed_before = (period - 1) * period_length\n",
    "            # Time elapsed within this period\n",
    "            elapsed_in_period = period_length - sec_left\n",
    "            elapsed_total = elapsed_before + elapsed_in_period\n",
    "            # Time left in regulation\n",
    "            return max(0, total_reg - elapsed_total)\n",
    "        else:\n",
    "            # In any OT, treat \"time remaining until game ends\" as just\n",
    "            # the time left in this OT period.\n",
    "            return sec_left\n",
    "\n",
    "    df[\"seconds_remaining_game\"] = df.apply(compute_seconds_remaining_game, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d72c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure scores are numeric, forward-filled, and add score_diff (home - away).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Some rows may have None / NaN scores; convert and forward fill.\n",
    "    df[\"scoreHome\"] = pd.to_numeric(df[\"scoreHome\"], errors=\"coerce\")\n",
    "    df[\"scoreAway\"] = pd.to_numeric(df[\"scoreAway\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"scoreHome\"] = df[\"scoreHome\"].ffill().fillna(0).astype(int)\n",
    "    df[\"scoreAway\"] = df[\"scoreAway\"].ffill().fillna(0).astype(int)\n",
    "\n",
    "    df[\"score_diff\"] = df[\"scoreHome\"] - df[\"scoreAway\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76780b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outcome_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a binary label 'home_win' for each row based on final score.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Look at the last non-null score row to determine result\n",
    "    final_home = df[\"scoreHome\"].iloc[-1]\n",
    "    final_away = df[\"scoreAway\"].iloc[-1]\n",
    "\n",
    "    home_win = int(final_home > final_away)\n",
    "    df[\"home_win\"] = home_win\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_home_away_team_ids(game_id: str):\n",
    "    summary = boxscoresummaryv2.BoxScoreSummaryV2(game_id=game_id)\n",
    "    game_summary = summary.game_summary.get_data_frame().iloc[0]\n",
    "\n",
    "    home_team_id = int(game_summary[\"HOME_TEAM_ID\"])\n",
    "    away_team_id = int(game_summary[\"VISITOR_TEAM_ID\"])\n",
    "    return home_team_id, away_team_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_free_throw_subtype(sub_type: str):\n",
    "    \"\"\"\n",
    "    Parse subType strings like:\n",
    "      'Free Throw 1 of 2'\n",
    "      'Free Throw 2 of 2'\n",
    "      'Free Throw Flagrant 1 of 2'\n",
    "      'Free Throw Flagrant 2 of 2'\n",
    "\n",
    "    Returns:\n",
    "      (idx, total) as ints, or (None, None) if no match.\n",
    "\n",
    "    If we later encounter patterns like 'Free Throw Technical' with no\n",
    "    'X of Y', we can treat them as (1,1).\n",
    "    \"\"\"\n",
    "    if not isinstance(sub_type, str):\n",
    "        return None, None\n",
    "\n",
    "    s = sub_type.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # normalize whitespace\n",
    "\n",
    "    # General pattern: \"Free Throw [optional word(s)] X of Y\"\n",
    "    # e.g. \"Free Throw 1 of 2\", \"Free Throw Flagrant 1 of 2\"\n",
    "    m = re.search(r\"Free Throw.*?(\\d+)\\s+of\\s+(\\d+)\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        idx = int(m.group(1))\n",
    "        total = int(m.group(2))\n",
    "        return idx, total\n",
    "\n",
    "    # Fallback: something like \"Free Throw Technical\" â†’ treat as 1 of 1\n",
    "    if \"free throw\" in s.lower():\n",
    "        return 1, 1\n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loc_to_sign(loc_value: str) -> int:\n",
    "    \"\"\"\n",
    "    Map PlayByPlayV3 'location' field to +1 (home), -1 (away), or 0 (neutral).\n",
    "\n",
    "    Adjust this mapping if your actual 'location' values differ.\n",
    "    \"\"\"\n",
    "    if loc_value is None or pd.isna(loc_value):\n",
    "        return 0\n",
    "    s = str(loc_value).strip().lower()\n",
    "\n",
    "    # Common patterns: 'home', 'away', but tweak if you see others\n",
    "    if s in (\"home\", \"hom\", \"h\"):\n",
    "        return 1\n",
    "    if s in (\"away\", \"awy\", \"visitor\", \"vis\", \"v\"):\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def add_free_throw_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add:\n",
    "      - home_ft_remaining\n",
    "      - away_ft_remaining\n",
    "\n",
    "    Semantics: at each row, these give the number of free throws still to be\n",
    "    taken by each team *after* this event.\n",
    "\n",
    "    Logic:\n",
    "      - Identify free throw rows by actionType containing 'Free Throw'.\n",
    "      - Parse subType ('Free Throw 1 of 2', 'Free Throw Flagrant 1 of 2', etc.)\n",
    "      - remaining = max(0, total - idx)\n",
    "      - If the shooter is home:  home_ft_remaining = remaining\n",
    "      - If the shooter is away: away_ft_remaining = remaining\n",
    "      - Non-FT rows keep the previous remaining values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure we have location_sign\n",
    "    if \"location_sign\" not in df.columns:\n",
    "        df[\"location_sign\"] = df[\"location\"].apply(_loc_to_sign)\n",
    "\n",
    "    action_types = df[\"actionType\"].fillna(\"\").astype(str)\n",
    "\n",
    "    home_ft_rem = 0\n",
    "    away_ft_rem = 0\n",
    "\n",
    "    home_ft_remaining = []\n",
    "    away_ft_remaining = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        atype = action_types.iat[i].lower()\n",
    "        loc_sign = df[\"location_sign\"].iat[i]\n",
    "\n",
    "        # If this row is a free throw, update the counters based on subType\n",
    "        if \"free throw\" in atype:\n",
    "            sub_type = df[\"subType\"].iat[i]\n",
    "            idx, total = parse_free_throw_subtype(sub_type)\n",
    "\n",
    "            if idx is not None and total is not None:\n",
    "                remaining = max(0, total - idx)\n",
    "                if loc_sign == 1:\n",
    "                    # Home shooting FTs\n",
    "                    home_ft_rem = remaining\n",
    "                elif loc_sign == -1:\n",
    "                    # Away shooting FTs\n",
    "                    away_ft_rem = remaining\n",
    "                # If loc_sign == 0, we ignore (weird neutral FT case)\n",
    "        # For non-FT rows, we just carry over current home_ft_rem / away_ft_rem\n",
    "\n",
    "        # Record the state *after* this event\n",
    "        home_ft_remaining.append(home_ft_rem)\n",
    "        away_ft_remaining.append(away_ft_rem)\n",
    "\n",
    "    df[\"home_ft_remaining\"] = home_ft_remaining\n",
    "    df[\"away_ft_remaining\"] = away_ft_remaining\n",
    "\n",
    "    # You can keep or drop location_sign depending on whether you use it elsewhere\n",
    "    # df.drop(columns=[\"location_sign\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_possession_after(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a 'possession' column representing who has the ball AFTER each event,\n",
    "    from the home team's perspective:\n",
    "\n",
    "        possession âˆˆ {+1, 0, -1}\n",
    "          +1 = home has ball\n",
    "          -1 = away has ball\n",
    "           0 = no clear possession (end of period, jump ball, game over, etc.)\n",
    "\n",
    "    Logic:\n",
    "      - Look ahead from each row to the next relevant action:\n",
    "          * Skip Substitution / Timeout rows\n",
    "          * If next is Foul: invert the location team\n",
    "          * If next is period / Jump Ball: set possession = 0\n",
    "          * Else: possession = location of that next row (home/away)\n",
    "      - For rows that are themselves period / Jump Ball: possession = 0\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Precompute a simple sign for each row's location\n",
    "    df[\"location_sign\"] = df[\"location\"].apply(_loc_to_sign)\n",
    "\n",
    "    n = len(df)\n",
    "    possession_after = [0] * n\n",
    "\n",
    "    action_types = df[\"actionType\"].fillna(\"\").astype(str)\n",
    "\n",
    "    for i in range(n):\n",
    "        atype_i = action_types.iat[i].lower()\n",
    "\n",
    "        # If this row itself is end-of-period or a jump ball, possession is 0\n",
    "        if \"period\" in atype_i or \"jump ball\" in atype_i:\n",
    "            possession_after[i] = 0\n",
    "            continue\n",
    "        if \"rebound\" in atype_i:\n",
    "            possession_after[i] = df[\"location_sign\"].iat[i]\n",
    "            continue\n",
    "        # Look ahead to find the next relevant offensive action\n",
    "        poss = 0\n",
    "        j = i + 1\n",
    "        while j < n:\n",
    "            atype_j = action_types.iat[j].lower()\n",
    "            loc_sign_j = df[\"location_sign\"].iat[j]\n",
    "\n",
    "            # Skip subs/timeouts when searching\n",
    "            if \"substitution\" in atype_j or \"timeout\" in atype_j:\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            # If the next significant event is end-of-period or jump ball â†’ no possession\n",
    "            if \"period\" in atype_j or \"jump ball\" in atype_j:\n",
    "                poss = 0\n",
    "                break\n",
    "\n",
    "            # If the next event is a foul, invert the fouling team to get offense\n",
    "            if \"foul\" in atype_j:\n",
    "                if loc_sign_j == 1:\n",
    "                    poss = -1\n",
    "                elif loc_sign_j == -1:\n",
    "                    poss = 1\n",
    "                else:\n",
    "                    poss = 0\n",
    "                break\n",
    "\n",
    "            # Otherwise, treat the next event's team as having the ball\n",
    "            if loc_sign_j != 0:\n",
    "                poss = loc_sign_j\n",
    "                break\n",
    "\n",
    "            # If location is neutral/unknown, keep scanning\n",
    "            j += 1\n",
    "\n",
    "        possession_after[i] = poss\n",
    "\n",
    "    df[\"possession\"] = possession_after\n",
    "    # Optionally drop helper\n",
    "    df.drop(columns=[\"location_sign\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_state_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only rows where we want to define a model state.\n",
    "\n",
    "    States are:\n",
    "      - After a MADE field goal\n",
    "      - After a rebound (except 'Normal Rebound' pseudo-events)\n",
    "      - After a turnover\n",
    "      - After dead-ball / possession-setting events (fouls, timeouts, violations, OOB, etc.)\n",
    "      - During free throws that are NON-REBOUNDABLE on a miss\n",
    "        (all pre-final FTs in a trip, plus flagrant/technical FTs)\n",
    "      - During reboundable FTs *only if made* (missed ones defer to the rebound row)\n",
    "\n",
    "    We DROP:\n",
    "      - Missed normal field goals (state comes at rebound/dead-ball)\n",
    "      - Missed reboundable FTs (state comes at rebound)\n",
    "      - Duplicate offensive-foul rows\n",
    "      - Fake 'Normal Rebound' rows after non-reboundable FTs\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    def _is_state_row(row) -> bool:\n",
    "        atype = str(row[\"actionType\"]).lower()\n",
    "        sub_raw = row.get(\"subType\", \"\")\n",
    "        sub = str(sub_raw).strip()\n",
    "        poss  = row[\"possession\"]\n",
    "        sub_type = str(sub_raw)\n",
    "        sub_lower = sub_type.lower()\n",
    "        if atype == \"made shot\":\n",
    "            return True\n",
    "        # 1) Remove offensive-charge duplicate foul rows\n",
    "        if sub == \"Offensive Charge\":\n",
    "            return False\n",
    "\n",
    "        # 2) Remove fake team rebounds after non-reboundable missed FTs\n",
    "        if sub == \"Normal Rebound\":\n",
    "            return False\n",
    "\n",
    "        # Only keep rows where we actually know who has the ball afterward\n",
    "        if poss == 0:\n",
    "            return False\n",
    "\n",
    "        desc = str(row.get(\"description\", \"\") or \"\")\n",
    "        desc_upper = desc.upper()\n",
    "\n",
    "        # --- FREE THROWS ---\n",
    "        if \"free throw\" in atype:\n",
    "\n",
    "            # Parse X of Y if present (normal FT trips)\n",
    "            idx, total = parse_free_throw_subtype(sub_type)\n",
    "\n",
    "            # Flagrant / technical FTs: treat as NON-REBOUNDABLE â†’ always keep\n",
    "            if \"flagrant\" in sub_lower or \"technical\" in sub_lower:\n",
    "                return True\n",
    "\n",
    "            # Normal FTs with 'X of Y'\n",
    "            if idx is not None and total is not None:\n",
    "                if idx < total:\n",
    "                    # Not the last FT in the trip â†’ NON-REBOUNDABLE (dead ball regardless)\n",
    "                    return True\n",
    "                else:\n",
    "                    # Last FT in a normal trip â†’ REBOUNDABLE\n",
    "                    # Use description: if it starts with 'MISS', it's a miss.\n",
    "                    is_miss = desc_upper.startswith(\"MISS\")\n",
    "                    if not is_miss:\n",
    "                        # Made last FT â†’ define a state here\n",
    "                        return True\n",
    "                    else:\n",
    "                        # Missed last FT â†’ let the rebound/dead-ball event define the state\n",
    "                        return False\n",
    "\n",
    "            # If we can't parse, safest assumption is: non-reboundable FT â†’ keep\n",
    "            return True\n",
    "\n",
    "        # --- FIELD GOALS (NON-FT) ---\n",
    "        is_fg_row = bool(row.get(\"isFieldGoal\", 0))\n",
    "        sr = str(row.get(\"shotResult\", \"\")).lower()\n",
    "\n",
    "        if is_fg_row:\n",
    "            # For non-FT FGs we can still use shotResult; it's usually reliable.\n",
    "            if \"made\" in sr:\n",
    "                return True          # made FG â†’ state here\n",
    "            if \"miss\" in sr:\n",
    "                return False         # missed FG â†’ state at rebound\n",
    "            return False             # unknown, drop\n",
    "\n",
    "        # --- REBOUNDS: possession-resolving event ---\n",
    "        if \"rebound\" in atype:\n",
    "            return True\n",
    "\n",
    "        # --- TURNOVERS: possession-resolving event ---\n",
    "        if \"turnover\" in atype:\n",
    "            return True\n",
    "\n",
    "        # --- DEAD-BALL / POSSESSION-SETTING EVENTS ---\n",
    "        if \"timeout\" in atype:\n",
    "            return True\n",
    "\n",
    "        # Will be cross-listed as turnover if relevant\n",
    "        if \"violation\" in atype:\n",
    "            return False\n",
    "\n",
    "        if \"out of bounds\" in atype or \"out-of-bounds\" in atype:\n",
    "            return True\n",
    "\n",
    "        # Fouls often lead to FTs or side-out; possession is effectively known\n",
    "        if \"foul\" in atype:\n",
    "            if \"offensive\" in sub_lower:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        # Period/jump ball: ambiguous/neutral â†’ skip as states\n",
    "        if \"period\" in atype or \"jump ball\" in atype:\n",
    "            return False\n",
    "        \n",
    "        # Everything else: non-state by default\n",
    "        return False\n",
    "\n",
    "    mask = df.apply(_is_state_row, axis=1)\n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clutch_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a boolean 'is_clutch' column.\n",
    "\n",
    "    Definition:\n",
    "      - True for all OT periods (period > 4).\n",
    "      - In regulation (period <= 4), true if:\n",
    "          * period == 4\n",
    "          * seconds_remaining_in_period <= 300 (last 5 minutes of 4Q)\n",
    "          * abs(score_diff) <= 5\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Make sure needed columns exist\n",
    "    if \"seconds_remaining_game\" not in df.columns:\n",
    "        raise ValueError(\"seconds_remaining_game column is required\")\n",
    "    if \"score_diff\" not in df.columns:\n",
    "        raise ValueError(\"score_diff column is required\")\n",
    "\n",
    "    period = df[\"period\"]\n",
    "    sec_left = df[\"seconds_remaining_game\"]\n",
    "    score_diff = df[\"score_diff\"].astype(float)\n",
    "\n",
    "    # OT: always clutch\n",
    "    in_ot = period > 4\n",
    "\n",
    "    # Regulation clutch: last 5 min of 4Q and close score\n",
    "    in_reg_clutch = (\n",
    "        (period == 4)\n",
    "        & (sec_left <= 300.0)\n",
    "        & (np.abs(score_diff) <= 5.0)\n",
    "    )\n",
    "\n",
    "    df[\"is_clutch\"] = (in_ot | in_reg_clutch).astype(int)  # or keep as bool\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_game_state_dataset_for_game(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = fix_scores_carry_forward(df)\n",
    "    df = add_time_features(df)\n",
    "    df = add_score_features(df)\n",
    "    df = add_possession_after(df)      # lookahead-based, training-time only\n",
    "    df = add_free_throw_features(df)\n",
    "    df = add_outcome_label(df)\n",
    "    df = add_clutch_feature(df)\n",
    "    \n",
    "\n",
    "    df = filter_to_state_rows(df)      # <-- drop missed-shot rows, etc.\n",
    "    cols = [\n",
    "        \"gameId\",\n",
    "        \"period\",\n",
    "        \"clock\",\n",
    "        \"seconds_remaining_game\",\n",
    "        \"is_clutch\",\n",
    "        \"scoreHome\",\n",
    "        \"scoreAway\",\n",
    "        \"score_diff\",\n",
    "        \"possession\",\n",
    "        \"home_ft_remaining\",\n",
    "        \"away_ft_remaining\",\n",
    "        \"home_win\",\n",
    "        \"actionType\",\n",
    "        \"subType\",\n",
    "        \"location\",\n",
    "        \"description\",\n",
    "        \"shotResult\",\n",
    "        \"isFieldGoal\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    return df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pbp_v3(game_id: str) -> pd.DataFrame:\n",
    "    path = os.path.join(PBP_OUT_DIR, f\"{game_id}.csv\") # Same as directory to save raw PBP data\n",
    "    print(\"Loading:\", path)\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5409ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_processed_odds(odds_csv_path: str) -> pd.DataFrame:\n",
    "    raw = pd.read_csv(odds_csv_path)\n",
    "    raw[\"z1\"] = raw.groupby(\"game_id\")[\"price1\"].transform(\n",
    "        lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    raw[\"z2\"] = raw.groupby(\"game_id\")[\"price2\"].transform(\n",
    "        lambda x: (x - x.mean()) / x.std()\n",
    "    )\n",
    "    raw = raw[raw[\"z1\"].abs() < 2]\n",
    "    raw = raw[raw[\"z2\"].abs() < 2]\n",
    "    raw.drop(['z1','z2'], axis=1, inplace=True)\n",
    "    \n",
    "    if \"game_id\" in raw.columns:\n",
    "        game_col = \"game_id\"\n",
    "    elif \"gameId\" in raw.columns:\n",
    "        game_col = \"gameId\"\n",
    "    else:\n",
    "        raise ValueError(\"Could not find game_id or gameId column in odds CSV.\")\n",
    "\n",
    "    df = raw.rename(columns={\n",
    "        game_col: \"gameId\",\n",
    "        \"price1\": \"home_ml\",\n",
    "        \"price2\": \"away_ml\",\n",
    "    })\n",
    "\n",
    "    grouped = df.groupby(\"gameId\").agg({\n",
    "        \"home_ml\": \"mean\",\n",
    "        \"away_ml\": \"mean\",\n",
    "    }).reset_index()\n",
    "\n",
    "    grouped[\"implied_prob_home\"] = grouped[\"home_ml\"].apply(american_to_implied_prob)\n",
    "    grouped[\"implied_prob_away\"] = grouped[\"away_ml\"].apply(american_to_implied_prob)\n",
    "    grouped[\"gameId\"] = (\n",
    "        grouped[\"gameId\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"\\D\", \"\", regex=True)\n",
    "        .str.zfill(10)\n",
    "    )\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cached_game_ids(pbp_dir: str) -> list[str]:\n",
    "    game_ids = []\n",
    "    for fname in os.listdir(pbp_dir):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        game_id = os.path.splitext(fname)[0]\n",
    "        game_ids.append(game_id)\n",
    "    return sorted(game_ids)\n",
    "\n",
    "def build_full_training_dataset_from_cache_serial(\n",
    "    pbp_dir: str,\n",
    "    odds_csv_path: str,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    print(\"Using pbp_dir:\", pbp_dir)\n",
    "    print(\"Files in pbp_dir:\", len(os.listdir(pbp_dir)))\n",
    "    print(os.listdir(pbp_dir)[:10])  # first 10 entries\n",
    "    \n",
    "    processed_odds = build_processed_odds(odds_csv_path)\n",
    "    \n",
    "    \n",
    "    cached_games = set(list_cached_game_ids(pbp_dir))\n",
    "    games_with_odds = set(processed_odds[\"gameId\"].astype(str))\n",
    "    usable_games = sorted(cached_games & games_with_odds)\n",
    "\n",
    "    print(f\"[INFO] Cached PBP games:      {len(cached_games)}\")\n",
    "    print(f\"[INFO] Games with odds:       {len(games_with_odds)}\")\n",
    "    print(f\"[INFO] Usable intersection:   {len(usable_games)}\")\n",
    "\n",
    "    odds_subset = processed_odds[processed_odds[\"gameId\"].isin(usable_games)]\n",
    "\n",
    "    dfs = []\n",
    "    for row in odds_subset.itertuples(index=False):\n",
    "        game_id = row.gameId\n",
    "        home_imp = row.implied_prob_home\n",
    "        away_imp = row.implied_prob_away\n",
    "\n",
    "        print(f\"[GAME] Processing {game_id}...\")\n",
    "\n",
    "        try:\n",
    "            states = build_game_state_dataset_for_game(game_id)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping game {game_id} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        states[\"home_win_implied_prob\"] = home_imp\n",
    "        states[\"away_win_implied_prob\"] = away_imp\n",
    "\n",
    "        dfs.append(states)\n",
    "\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"No games were successfully processed from cache (serial).\")\n",
    "\n",
    "    training_df = pd.concat(dfs, ignore_index=True)\n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"DataFrameGroupBy.apply\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2956d813",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Converted to raw in order to prevent accidentally running large data processing cells\n",
    "\n",
    "training_df = build_full_training_dataset_from_cache_serial(\n",
    "    pbp_dir=PBP_OUT_DIR,\n",
    "    odds_csv_path=ADJUSTED_ODDS_CSV_PATH,\n",
    ")\n",
    "print(training_df.shape)\n",
    "training_df.to_csv(\"full_training_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f8d6a",
   "metadata": {},
   "source": [
    "## ðŸ§  Model Creation and Training\n",
    "\n",
    "This section builds and trains the neural-network win probability model using the engineered game-state dataset. The model is designed to estimate the probability that the home team wins the game given the score, time remaining, possession, free throws, betting priors, and other contextual features.\n",
    "\n",
    "Key steps include:\n",
    "\n",
    "- **Defining the model architecture:**  \n",
    "  A lightweight multilayer perceptron (MLP) implemented in PyTorch, with nonlinear activations and dropout for regularization.\n",
    "\n",
    "- **Preparing the training data:**  \n",
    "  Scaling continuous variables, selecting the final feature set, and grouping rows by game to avoid leakage in cross-validation.\n",
    "\n",
    "- **Training the model:**  \n",
    "  Using the Adam optimizer with binary cross-entropy loss, monitoring validation performance, and tuning training duration to prevent overfitting.\n",
    "\n",
    "- **Calibrating probabilities:**  \n",
    "  Raw neural network outputs are passed through an Isotonic Regression calibration layer to produce accurate and well-behaved win probabilities.\n",
    "\n",
    "- **Evaluating performance:**  \n",
    "  Computing Brier score, log loss, and ROC AUC on validation data to assess both accuracy and calibration quality.\n",
    "\n",
    "This stage produces the final trained model (`nn_winprob_model.pt`) and the auxiliary objects (`nn_winprob_aux.pkl`) that power the real-time win probability generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load training data\n",
    "df = pd.read_csv(\"full_training_dataset.csv\")\n",
    "\n",
    "# ---- Adjust this if your column is named differently ----\n",
    "TARGET_COL = \"home_win\"          # 1 if home wins, 0 if away wins\n",
    "GAME_ID_COL = \"gameId\"           # or \"game_id\", etc.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Time transforms for better behavior\n",
    "if \"seconds_remaining_game\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'seconds_remaining_game' in df for time-based features.\")\n",
    "\n",
    "df[\"t_log\"] = np.log1p(df[\"seconds_remaining_game\"])\n",
    "df[\"t_inv\"] = 1.0 / (1.0 + df[\"seconds_remaining_game\"])\n",
    "\n",
    "if GAME_ID_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected a game id column '{GAME_ID_COL}' in df.columns\")\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected target column '{TARGET_COL}' in df.columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77df135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time transforms for better behavior\n",
    "if \"seconds_remaining_game\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'seconds_remaining' in df for time-based features.\")\n",
    "\n",
    "df[\"t_log\"] = np.log1p(df[\"seconds_remaining_game\"])          # smooth log time\n",
    "df[\"t_inv\"] = 1.0 / (1.0 + df[\"seconds_remaining_game\"])      # high sensitivity for late game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5497a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_decay_weights(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Higher weight for later-game states.\n",
    "    Assumes df has a 'seconds_remaining' column (bigger = earlier in game).\n",
    "    Optionally boosts 4th quarter if 'period' exists.\n",
    "    \"\"\"\n",
    "    if \"seconds_remaining_game\" not in df.columns:\n",
    "        raise ValueError(\"Need a 'seconds_remaining_game' column to compute time-decay weights.\")\n",
    "\n",
    "    t = df[\"seconds_remaining_game\"].astype(float)\n",
    "\n",
    "    # Scale so earliest times get weight ~1, latest get ~2\n",
    "    t_max = t.max()\n",
    "    w = 1.0 + (t_max - t) / t_max   # ranges roughly from 1.0 to 2.0\n",
    "\n",
    "    # Optional: extra emphasis on 4th quarter and OT\n",
    "    if \"period\" in df.columns:\n",
    "        w = w * (1.0 + 0.5 * (df[\"period\"] >= 4))\n",
    "\n",
    "    return w\n",
    "\n",
    "weights = compute_time_decay_weights(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define X, y\n",
    "y = df[TARGET_COL].astype(int)\n",
    "groups = df[GAME_ID_COL].astype(str)\n",
    "\n",
    "# Drop non-feature columns (ids, text, etc.)\n",
    "drop_cols = {TARGET_COL, GAME_ID_COL}\n",
    "\n",
    "# Add anything else you *know* should not be a feature:\n",
    "drop_cols |= {\n",
    "    \"event_id\", \"eventNum\", \"description\", \"actionType\", \"subType\", \"clock\", \"period\", 'location', 'shotResult', 'isFieldGoal'\n",
    "} & set(df.columns)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "print(\"Using\", len(feature_cols), \"features:\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d136e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "w_train = weights.iloc[train_idx].to_numpy()\n",
    "w_val   = weights.iloc[val_idx].to_numpy()  # usually not used, but available if needed\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "print(\"Train rows:\", len(X_train), \"Val rows:\", len(X_val))\n",
    "print(\"Train games:\", len(np.unique(groups.iloc[train_idx])),\n",
    "      \"Val games:\", len(np.unique(groups.iloc[val_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5533ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 1) Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "# 2) PyTorch Dataset\n",
    "class NNDataset(Dataset):\n",
    "    def __init__(self, X, y, w=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        if hasattr(y, \"values\"):\n",
    "            y = y.values\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        if w is None:\n",
    "            self.w = torch.ones_like(self.y)\n",
    "        else:\n",
    "            if hasattr(w, \"values\"):\n",
    "                w = w.values\n",
    "            self.w = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "train_ds = NNDataset(X_train_scaled, y_train, w_train)\n",
    "val_ds   = NNDataset(X_val_scaled, y_val, np.ones_like(y_val))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=1024, shuffle=False)\n",
    "\n",
    "# 3) Define the MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # shape (batch,)\n",
    "\n",
    "model = MLP(input_dim=X_train_scaled.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")  # we'll apply weights manually\n",
    "\n",
    "# 4) Training loop with sample weights + simple early stopping\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 5\n",
    "epochs_no_improve = 0\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    n_train = 0\n",
    "\n",
    "    for xb, yb, wb in train_loader:\n",
    "        xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss_raw = criterion(logits, yb)      # (batch,)\n",
    "        loss = (loss_raw * wb).mean()        # apply sample weights\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        n_train += xb.size(0)\n",
    "\n",
    "    train_loss /= n_train\n",
    "\n",
    "    # validation loss (unweighted, just for early stopping)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    n_val = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, wb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss_raw = criterion(logits, yb)\n",
    "            loss = loss_raw.mean()\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            n_val += xb.size(0)\n",
    "    val_loss /= n_val\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Load best weights\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "# 5) Uncalibrated validation metrics\n",
    "model.eval()\n",
    "val_logits_list = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb, wb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        val_logits_list.append(logits.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_logits = torch.cat(val_logits_list)\n",
    "val_proba_raw = torch.sigmoid(val_logits).numpy().flatten()\n",
    "\n",
    "print(\"Uncalibrated PyTorch NN metrics:\")\n",
    "print(\"  Brier:\", brier_score_loss(y_val, val_proba_raw))\n",
    "print(\"  Log loss:\", log_loss(y_val, val_proba_raw))\n",
    "print(\"  ROC AUC:\", roc_auc_score(y_val, val_proba_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "\n",
    "# Fit isotonic reg on val-set raw probs vs true labels\n",
    "iso.fit(val_proba_raw, y_val.to_numpy())\n",
    "\n",
    "val_proba_cal = iso.transform(val_proba_raw)\n",
    "\n",
    "print(\"Calibrated PyTorch NN metrics:\")\n",
    "print(\"  Brier:\", brier_score_loss(y_val, val_proba_cal))\n",
    "print(\"  Log loss:\", log_loss(y_val, val_proba_cal))\n",
    "print(\"  ROC AUC:\", roc_auc_score(y_val, val_proba_cal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee75202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit file paths as needed\n",
    "\n",
    "model_bundle = {\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"game_id_col\": GAME_ID_COL,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"model\": val_proba_cal,\n",
    "}\n",
    "torch.save(model.state_dict(), \"nn_winprob_model.pt\")\n",
    "aux_bundle = {\n",
    "    \"scaler\": scaler,\n",
    "    \"iso\": iso,\n",
    "    \"feature_cols\": feature_cols,\n",
    "}\n",
    "joblib.dump(aux_bundle, \"nn_winprob_aux.pkl\")\n",
    "joblib.dump(model_bundle, \"nba_winprob_nn_calibrated.pkl\")\n",
    "print(\"Saved model to nba_winprob_nn_calibrated.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba_wp_nc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
